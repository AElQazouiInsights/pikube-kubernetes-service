---
title: Monitoring with Prometheus
permalink: /docs/9-monitoring/7-monitoring-prometheus
description: How to deploy kuberentes cluster monitoring solution based on Prometheus. Installation based on Prometheus Operator using kube-prometheus-stack project.
last_modified_at: "09-12-2024"
---

# {{ $frontmatter.title }}

<p align="center">
    <img alt="prometheus"
    src="../resources/monitoring/prometheus-logo.svg"
    width="80%"
    height="%">
</p>

<!-- - [{{ $frontmatter.title }}](#-frontmattertitle-)
  - [About Prometheus Operator](#about-prometheus-operator)
  - [Kube-Prometheus Stack Installation via Helm Chart](#kube-prometheus-stack-installation-via-helm-chart)
  - [Configuring Ingress Resources for Accessing Monitoring Services](#configuring-ingress-resources-for-accessing-monitoring-services)
  - [Detailed Breakdown of what been deployed by kube-stack](#detailed-breakdown-of-what-been-deployed-by-kube-stack)
    - [Prometheus Operator](#prometheus-operator)
      - [Detailed Breakdown of Prometheus Custom Resource](#detailed-breakdown-of-prometheus-custom-resource)
      - [Detailed Breakdown of AlertManager Custom Resource](#detailed-breakdown-of-alertmanager-custom-resource)
      - [Detailed Breakdown of ServiceMonitor Objects](#detailed-breakdown-of-servicemonitor-objects)
      - [Detailed Breakdown of PrometheusRule Objects](#detailed-breakdown-of-prometheusrule-objects)
    - [Grafana](#grafana)
      - [Keycloak Integration: Single Sign-On Configuration](#keycloak-integration-single-sign-on-configuration)
        - [Keycloak Configuration: Configure Grafana Client](#keycloak-configuration-configure-grafana-client)
    - [Configuring SSO for Grafana SSO](#configuring-sso-for-grafana-sso)
    - [Provisioning Grafana Dashboards Automatically](#provisioning-grafana-dashboards-automatically)
    - [Automatic Provisioning of Grafana DataSources](#automatic-provisioning-of-grafana-datasources)
    - [Prometheus Node Exporter Deployment](#prometheus-node-exporter-deployment)
    - [Deployment of Kube State Metrics](#deployment-of-kube-state-metrics)
  - [Monitoring K3S and Cluster Services](#monitoring-k3s-and-cluster-services)
    - [K3S Components Monitoring](#k3s-components-monitoring)
      - [coreDNS Monitoring](#coredns-monitoring)
      - [Integrate and Modify Grafana Dashboards with K3S](#integrate-and-modify-grafana-dashboards-with-k3s)
    - [Monitoring Ingress NGINX](#monitoring-ingress-nginx)
      - [Ingress NGINX Grafana Dashboard](#ingress-nginx-grafana-dashboard)
    - [Monitoring Longhorn](#monitoring-longhorn)
      - [Longhorn Grafana Dashboard](#longhorn-grafana-dashboard)
    - [Monitoring Minio](#monitoring-minio)
      - [Minio Grafana Dashboard](#minio-grafana-dashboard)
    - [Elasticsearch Monitoring](#elasticsearch-monitoring)
      - [Elasticsearch Grafana Dashboard](#elasticsearch-grafana-dashboard) -->

The installation of a **`Prometheus`** stack for Kubernetes can be efficiently conducted using the [**`kube-prometheus`**](https://github.com/prometheus-operator/kube-prometheus) project, which is maintained by the community.

This project aggregates Kubernetes manifests, Grafana dashboards, and Prometheus rules, along with comprehensive documentation and scripts. This collection is designed to facilitate a straightforward operational experience for end-to-end Kubernetes cluster monitoring with Prometheus, leveraging the Prometheus Operator.

**The kube-prometheus stack includes the following components**:

- [**`Prometheus Operator`**](https://github.com/prometheus-operator/prometheus-operator): Streamlines the deployment, configuration, and management of Prometheus and related monitoring components.

- [**`Highly Available Prometheus`**](https://prometheus.io/): Ensures Prometheus is consistently up and running, minimizing downtime and data loss.

- [**`Highly Available Alertmanager`**](https://github.com/prometheus/alertmanager): Manages and dispatches alerts generated by Prometheus, ensuring high availability.

- [**`Prometheus Node Exporter`**](https://github.com/prometheus/node_exporter): Gathers metrics from each node in the cluster, providing a detailed view of node-level statistics.

- [**`Kube-State-Metrics`**](https://github.com/kubernetes/kube-state-metrics): Collects metrics about the state of various Kubernetes objects, offering insights into the operational status of different components.

- [**`Grafana`**](https://grafana.com/): A powerful visualization tool that integrates with Prometheus to create insightful dashboards reflecting the cluster's health and performance.

This stack is specifically tailored for cluster monitoring, pre-configured to gather metrics from all Kubernetes components comprehensively.

Additionally, the architecture of the deployed components is illustrated in the below diagram, offering a visual understanding of how the system operates and interacts within a Kubernetes environment. This visual aid enhances the comprehensibility of the monitoring setup and its constituent elements.

## About Prometheus Operator

The Prometheus Operator is designed to manage Prometheus and AlertManager deployments along with their configurations, utilizing Kubernetes CRDs (Custom Resource Definitions):

- **`Prometheus and AlertManager CRDs`**: These allow for the declarative specification of the desired setup for Prometheus and AlertManager within a Kubernetes cluster. They offer configuration options for aspects such as the number of replicas and persistent storage.

- **`ServiceMonitor, PodMonitor, and Probe CRDs`**: These manage the service discovery configurations for Prometheus. They define the way a dynamic set of services, pods, or static targets should be monitored, enhancing the flexibility and scalability of monitoring.

- **`PrometheusRules CRD`**: This defines the alerting and recording rules for Prometheus. Alerting rules specify the conditions under which alerts should be generated and sent (via AlertManager), while recording rules allow for the pre-computation and storage of frequently needed or complex expressions as a new set of time series data.

- **`AlertManagerConfig CRD:`** This defines the configuration for AlertManager, enabling the routing of alerts to customized receivers and the establishment of inhibition rules, which control when certain alerts should be suppressed.

📌 **Note**

*For comprehensive details about the design and functionality of the Prometheus Operator and its CRDs, refer to the [**`Prometheus Operator Design Documentation`**](https://prometheus-operator.dev/docs/operator/design/).*

*The specifications for the various CRDs are detailed in the [**`Prometheus Operator API`**](https://prometheus-operator.dev/docs/operator/api/) reference guide, providing in-depth information for advanced configuration and usage.*

## Kube-Prometheus Stack Installation via Helm Chart

The Kube-Prometheus stack, maintained by the community, provides a comprehensive collection of monitoring resources, and it can be efficiently deployed using the Helm package manager.

- Start by adding the official Prometheus community repository to Helm

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
```

- Fetch the latest charts and updates from all added repositories

```bash
helm repo update
```

- Establish a dedicated namespace for all your monitoring components

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml create namespace monitoring
```

- Create **`prometeus-values.yaml`** file with the desired configuration for Kube-Prometheus stack. Below is a detailed breakdown of the key configurations:

  - **Prometheus Operator**:

  ➜ ServiceMonitor: Configures the relabeling of metrics and disables the kubelet service.

  - **Alertmanager**:

  ➜ Sets up a persistent volume using Longhorn and configures Alertmanager to run under a specific subpath. Routes and storage specifications are defined.

  **Prometheus**:

  ➜ Details memory resources, storage, and retention period. Configures Prometheus to run under a subpath and removes default selector filters to allow more flexible monitoring setups.

  - **Grafana**:

  ➜ Configures the domain, subpath, admin password, and plugins. Sets up service monitor labeling and relabeling. Integrates with Loki and configures the dashboard sidecar for searching across all namespaces.

  - **Kubernetes Component Monitoring**:

  ➜ Disables monitoring for default Kubernetes components since K3S components will be monitored separately.

  - **Service Monitors and Selectors**:

  ➜ Adjusts default filters and relabeling for Prometheus, Prometheus Operator, and Grafana monitoring.

```yaml
# Prometheus Operator configuration
prometheusOperator:
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: prometheus-operator
  kubeletService:
    enabled: false

# Alertmanager configuration
alertmanager:
  alertmanagerSpec:
    externalUrl: http://monitor.picluster.quantfinancehub.com/alertmanager/
    routePrefix: /
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: alertmanager

# Prometheus configuration
prometheus:
  prometheusSpec:
    externalUrl: http://monitoring.picluster.quantfinancehub.com/prometheus/
    routePrefix: /
    resources:
      requests:
        memory: 1Gi
      limits:
        memory: 1Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    retention: 7d
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
  serviceMonitor:
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: prometheus

# Grafana configuration
grafana:
  grafana.ini:
    server:
      domain: monitoring.picluster.quantfinancehub.com
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      serve_from_sub_path: true
  adminPassword: "admin_password"
  plugins:
    - grafana-piechart-panel
  serviceMonitor:
    labels:
      release: kube-prometheus-stack
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: grafana
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki-gateway.logging.svc.cluster.local
  sidecar:
    dashboards:
      searchNamespace: ALL

# Disabling default Kubernetes service monitoring
kubelet: { enabled: false }
kubeApiServer: { enabled: false }
kubeControllerManager: { enabled: false }
kubeScheduler: { enabled: false }
kubeProxy: { enabled: false }
kubeEtcd: { enabled: false }

# Disabling default K8S Prometheus Rules
defaultRules:
  create: true
  rules:
    etcd: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeScheduler: false
```

- Deploy the **`Kube-Prometheus`** stack in the **`monitoring`** namespace using the Helm chart and the custom **`prometeus-values.yaml`**

```bash
helm --kubeconfig=/home/pi/.kube/config.yaml install -f prometeus-values.yaml kube-prometheus-stack prometheus-community/kube-prometheus-stack --namespace monitoring
```

## Configuring Ingress Resources for Accessing Monitoring Services

Facilitate external access to Prometheus, Grafana, and AlertManager services through an Ingress Controller. Instead of employing separate DNS domains for each component, they are configured to operate behind an NGINX HTTP Proxy using a single domain, monitoring.picluster.quantfinancehub.com, with unique subpaths:

**`Grafana`**:

https://monitoring.picluster.quantfinancehub.com/grafana/

**`Prometheus`**:

https://monitoring.picluster.quantfinancehub.com/prometheus/

**`AlertManager`**:

https://monitoring.picluster.quantfinancehub.com/alertmanager/

The DNS domain **`monitoring.picluster.quantfinancehub.com`** must be mapped in the cluster DNS server configuration to the external IP of the NGINX Load Balancer service.

As the backend services for Prometheus, Grafana, and AlertManager don't provide secure communications (HTTP traffic) by default, the Ingress resource will be configured to enforce HTTPS (NGINX TLS endpoint) and redirect all HTTP traffic to HTTPS. Additionally, since Prometheus and AlertManager frontends lack native authentication mechanisms, NGINX HTTP basic authentication will be employed for security.

Ingress NGINX rewrite rules are defined within the Ingress resources.

- Create Ingress Resources Manifest **`prometheus-monitoring-ingress.yaml`**

```yaml
# Ingress for Grafana
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-grafana
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /grafana/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-grafana
                port:
                  number: 80
---
# Ingress for Prometheus
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-prometheus
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: nginx/basic-auth-secret
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /prometheus/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-prometheus
                port:
                  number: 9090
---
# Ingress for AlertManager
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-alertmanager
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    cert-manager.io/cluster-issuer: letsencrypt-issuer
    cert-manager.io/common-name: monitoring.picluster.quantfinancehub.com
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - monitoring.picluster.quantfinancehub.com
      secretName: monitoring-tls
  rules:
    - host: monitoring.picluster.quantfinancehub.com
      http:
        paths:
          - path: /alertmanager/(.*)
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-stack-alertmanager
                port:
                  number: 9093
```

- Deploy the Ingress configuration to your cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml apply -f prometheus-monitoring-ingress.yaml
```

## Detailed Breakdown of what been deployed by kube-stack

### Prometheus Operator

#### Detailed Breakdown of Prometheus Custom Resource

The kube-stack deployment method efficiently sets up a Prometheus Operator in PiKube Kubernetes cluster. This process involves deploying the Prometheus Operator and creating the necessary **`Prometheus`** and **`AlertManager`** objects. These objects instruct the operator to roll out the corresponding Prometheus and AlertManager pods as StatefulSets.

📌 **Note**

*The final configuration can be customized through helm chart values (**`prometheus.prometheusSpec`** and **`alertmanager.alertmanagerSpec`**).*

The **`Prometheus custom resource`** (CR) is a crucial part of this setup. It contains the desired configuration for the Prometheus server.

It can be retrieved from PiKube Kubernetes Cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get prometheus -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get prometheus kube-prometheus-stack-prometheus -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:48Z"
  generation: 1
  labels:
    app: kube-prometheus-stack-prometheus
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-prometheus
  namespace: monitoring
  resourceVersion: "1378214"
  uid: 73845123-041c-4b2e-bf15-b64f40b0a6ed
spec:
  alerting:
    alertmanagers:
    - apiVersion: v2
      name: kube-prometheus-stack-alertmanager
      namespace: monitoring
      pathPrefix: /
      port: http-web
  enableAdminAPI: false
  evaluationInterval: 30s
  externalUrl: http://monitoring.picluster.quantfinancehub.com/prometheus/
  hostNetwork: false
  image: quay.io/prometheus/prometheus:v2.48.1
  listenLocal: false
  logFormat: logfmt
  logLevel: info
  paused: false
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  portName: http-web
  probeNamespaceSelector: {}
  probeSelector: {}
  replicas: 1
  resources:
    limits:
      memory: 1Gi
    requests:
      memory: 1Gi
  retention: 7d
  routePrefix: /
  ruleNamespaceSelector: {}
  ruleSelector: {}
  scrapeConfigNamespaceSelector: {}
  scrapeConfigSelector:
    matchLabels:
      release: kube-prometheus-stack
  scrapeInterval: 30s
  securityContext:
    fsGroup: 2000
    runAsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
    seccompProfile:
      type: RuntimeDefault
  serviceAccountName: kube-prometheus-stack-prometheus
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  shards: 1
  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: longhorn
  tsdb:
    outOfOrderTimeWindow: 0s
  version: v2.48.1
  walCompression: true
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2023-12-22T20:58:40Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Available
  - lastTransitionTime: "2023-12-22T20:58:40Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Reconciled
  paused: false
  replicas: 1
  shardStatuses:
  - availableReplicas: 1
    replicas: 1
    shardID: "0"
    unavailableReplicas: 0
    updatedReplicas: 1
  unavailableReplicas: 0
  updatedReplicas: 1
```

**Specifications of the Prometheus Custom Resource**:

**`apiVersion & kind`**: These fields identify the resource as a Prometheus object under the monitoring.coreos.com/v1 API.

**`metadata`**: This section provides the naming and labeling information for the resource, crucial for identification and management.

**`spec`**: This section is the heart of the resource, defining how Prometheus should run and behave:

**`alerting`**: Outlines how alerts should be managed and where they should be sent, specifying the AlertManager service details.

**`version & image`**: Specifies the Prometheus version and the container image to use, ensuring you're deploying a specific, known version.

**`replicas & shards`**: Dictates the high availability configuration, with replicas for redundancy and shards for distributing load.

**`scrapeInterval & evaluationInterval`**: Defines how often Prometheus scrapes targets and evaluates rules, crucial for timely data collection and alerting.

**`retention`**: Determines how long data is kept, balancing between historical insight and storage utilization.

**`storage`**: Specifies the persistent storage configuration, vital for data persistence across pod restarts and upgrades.

**`serviceMonitorSelector, podMonitorSelector, probeSelector`**: These selectors determine which monitor objects Prometheus should pay attention to, based on their labels.

> 💡 **Additional Considerations**
>
> **`HA Mechanisms`**: While basic HA is achieved through replication (running multiple instances), advanced > setups might use sharding. However, sharding requires additional components like Thanos for comprehensive querying and rule evaluation.

**`Resource and Storage Needs`**: Given the resource-intensive nature of Prometheus, especially in larger environments, ensure the cluster has the necessary resources and storage configured.

**`Alerting and Monitoring Scope`**: The setup connects to a specific AlertManager and monitors targets based on label selectors. Adjust these to fit the exact scope of the wanted monitoring and alerting on.

**`Customization and Flexibility`**: The default setup is designed for a general use case. However, there is possibility and a need to modify the helm values and resource definitions to tailor the setup to the specific needs and constraints.

#### Detailed Breakdown of AlertManager Custom Resource

The kube-stack deployment process not only sets up the Prometheus Operator but also creates and configures the necessary AlertManager objects. These objects guide the operator to deploy AlertManager pods as StatefulSets, ensuring a robust and responsive alerting system.

The AlertManager custom resource (CR) defines the desired state and configuration of the AlertManager server.

It can be retreived from PiKube Kunbernetes cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get alertmanager -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get alertmanager <name-of-alertmanager-cr> -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1                                                     
kind: Alertmanager                                                                       
metadata:                                                                                
  annotations:                                                                           
    meta.helm.sh/release-name: kube-prometheus-stack                                     
    meta.helm.sh/release-namespace: monitoring                                           
  creationTimestamp: "2023-12-22T19:54:48Z"                                              
  generation: 1                                                                          
  labels:                                                                                
    app: kube-prometheus-stack-alertmanager                                              
    app.kubernetes.io/instance: kube-prometheus-stack                                    
    app.kubernetes.io/managed-by: Helm                                                   
    app.kubernetes.io/part-of: kube-prometheus-stack                                     
    app.kubernetes.io/version: 55.5.0                                                    
    chart: kube-prometheus-stack-55.5.0                                                  
    heritage: Helm                                                                       
    release: kube-prometheus-stack                                                       
  name: kube-prometheus-stack-alertmanager                                               
  namespace: monitoring                                                                  
  resourceVersion: "1345806"                                                             
  uid: 763c17c3-8709-47fb-a839-2cc0310aa8fb                                              
spec:                                                                                    
  alertmanagerConfigNamespaceSelector: {}                                                
  alertmanagerConfigSelector: {}                                                         
  externalUrl: http://monitor.picluster.quantfinancehub.com/alertmanager/                
  image: quay.io/prometheus/alertmanager:v0.26.0                                         
  listenLocal: false                                                                     
  logFormat: logfmt                                                                      
  logLevel: info                                                                         
  paused: false                                                                          
  portName: http-web                                                                     
  replicas: 1                                                                            
  retention: 120h                                                                        
  routePrefix: /                                                                         
  securityContext:                                                                       
    fsGroup: 2000                                                                        
    runAsGroup: 2000                                                                     
    runAsNonRoot: true                                                                   
    runAsUser: 1000                                                                      
    seccompProfile:                                                                      
      type: RuntimeDefault                                                               
  serviceAccountName: kube-prometheus-stack-alertmanager                                 
  storage:                                                                               
    volumeClaimTemplate:                                                                 
      spec:                                                                              
        accessModes:                                                                     
        - ReadWriteOnce                                                                  
        resources:                                                                       
          requests:                                                                      
            storage: 50Gi                                                                
        storageClassName: longhorn                                                       
  version: v0.26.0                                                                       
status:                                                                                  
  availableReplicas: 1                                                                   
  conditions:                                                                            
  - lastTransitionTime: "2023-12-22T19:56:39Z"                                           
    message: ""                                                                          
    observedGeneration: 1                                                                
    reason: ""                                                                           
    status: "True"                                                                       
    type: Available                                                                      
  - lastTransitionTime: "2023-12-22T19:55:04Z"                                           
    message: ""                                                                          
    observedGeneration: 1                                                                
    reason: ""                                                                           
    status: "True"                                                                       
    type: Reconciled                                                                     
  paused: false                                                                          
  replicas: 1                                                                            
  unavailableReplicas: 0                                                                 
  updatedReplicas: 1
```

**Specifications of the AlertManager Custom Resource**:

**`apiVersion & kind`**: Specifies the resource type as Alertmanager under the monitoring.coreos.com/v1 API.

**`metadata`**: Provides naming and labeling details essential for resource management and identification.

**`spec`**: Describes how AlertManager should run and behave:

**`version & image`**: Dictates the AlertManager version and image, promoting consistency and reliability.

**`replicas`**: Indicates the number of AlertManager instances, pivotal for high availability.

**`retention`**: Sets how long AlertManager retains its data, balancing between availability and storage management.

**`storage`**: Outlines the persistent storage configuration to ensure data durability.

**`externalUrl`**: Specifies the external URL where AlertManager is accessible, vital for user interaction and integrations.

> 💡 **Additional Considerations**
>
> **`High Availability`**: The configuration can be scaled up to multiple replicas to enhance reliability and ensure continuous monitoring.
>
> **`Storage Requirements`**: Given AlertManager's role in managing alerts, ensure the cluster has the necessary resources and storage as defined.
>
> **`Integration with Prometheus`**: AlertManager works in tandem with Prometheus to handle alerts efficiently. Ensure it's correctly linked to the Prometheus instances.

#### Detailed Breakdown of ServiceMonitor Objects

**`The kube-prometheus-stack`** deployment automatically creates several ServiceMonitor objects. These objects are crucial as they define how Prometheus should discover and scrape metrics from various components:

**`Core Components`**: Includes **`Node Exporter`**, **`Grafana`**, **`Kube-State-Metrics`**, **`Prometheus`** itself, **`Prometheus Operator`** and **`AlertManager`**.

**`Kubernetes Services and Processes`**: Depending on the helm chart configuration, it might include **`coreDNS`**, **`Kube Api server`**, **`kubelet`**, **`Kube Controller Manager`**, **`Kubernetes Scheduler`**, **`Kubernetes etcd`**, and **`Kube Proxy`**.

> 📌 **Note**
>
> *In the chart's configuration, monitoring for the **`kube-controller-manager`**, **`kube-scheduler`**, **`kube-proxy`**, and **`kubelet`** components has been deactivated. However, the **`coreDNS`** component's monitoring remains active.*
>
> *Refer to the below [**`K3S components monitoring`**](../4-kubernetes/1-k3s-installation.md) section to understand the rationale behind the disabling of Kubernetes components monitoring in the kube-prometheus-stack and for guidance on how to manually set up monitoring for K3s.*

#### Detailed Breakdown of PrometheusRule Objects

Alongside ServiceMonitors, the **`kube-prometheus-stack`** generates PrometheusRule objects. These define the rules under which alerts are generated and the metrics Prometheus records:

**`Generated Rules`**: Include a comprehensive set of rules covering various aspects of the cluster's health and performance.

**`Customization`**: Rules can be added or modified to suit the specific monitoring and alerting needs.

### Grafana

Grafana is deployed as a subchart of the kube-prometheus-stack, and its configuration is passed via the [**`Grafana value`**](https://github.com/grafana/helm-charts/tree/main/charts/grafana) in the helm chart.

It can be retrieved from PiKube Kubernetes cluster

```bash
helm --kubeconfig=/home/pi/.kube/config.yaml list -n monitoring
helm --kubeconfig=/home/pi/.kube/config.yaml get values kube-prometheus-stack -n monitoring
```

```yaml
grafana:
  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki-gateway.logging.svc.cluster.local
  adminPassword: admin_password
  grafana.ini:
    server:
      domain: monitoring.picluster.quantfinancehub.com
      root_url: '%(protocol)s://%(domain)s:%(http_port)s/grafana/'
      serve_from_sub_path: true
  plugins:
  - grafana-piechart-panel
  serviceMonitor:
    labels:
      release: kube-prometheus-stack
    relabelings:
    - action: replace
      replacement: grafana
      sourceLabels:
      - __address__
      targetLabel: job
  sidecar:
    dashboards:
      searchNamespace: ALL
kubeApiServer:
  enabled: false
kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: false
kubeProxy:
  enabled: false
kubeScheduler:
  enabled: false
kubelet:
  enabled: false
```

Key configurations include:

**`Frontend Setup`**: Configured to run behind an HTTP proxy at the /grafana subpath.

**`Authentication`**: Specifies the admin password for secure access.

**`Plugins`**: Includes essential plugins like the grafana-piechart-panel.

**`Additional Data Sources`**: Integrates additional data sources such as Loki for log aggregation.

**`ServiceMonitor & Sidecar Configuration`**: Sets up ServiceMonitor labels and job relabeling along with additional sidecar dashboard provisioner configurations.

> 💡 **Additional Considerations**
>
> **`Deployment Customization`**: Both AlertManager and Grafana offer numerous customization options. Tailor these settings to align with the operational requirements and constraints.
>
> **`Integration and Verification`**: Once deployed, ensure all components are correctly integrated and functioning as expected. Regular checks and updates can help maintain a healthy monitoring environment.
>
> **`Resource Management`**: Monitor the resource usage and performance of AlertManager and Grafana, adjusting configurations as necessary to ensure optimal operation.

#### Keycloak Integration: Single Sign-On Configuration

Grafana can be integrated with an Identity and Access Management (IAM) solution, Keycloak, to enable Single Sign-On (SSO) functionality.

**Prerequisite**: Ensure that Keycloak is installed by following the procedure described in [**`SSO with KeyCloak and Oauth2-Proxy`**](../7-single-sign-on/1-sso-with-keycloak-and-oauth2-proxy.md)

##### Keycloak Configuration: Configure Grafana Client

To enable SSO, the Grafana client application must be configured within the picluster realm in Keycloak.

Procedure Reference: Follow the steps outlined in [**`Keycloak Documentation: Creating an OpenID Connect Client`**](https://www.keycloak.org/docs/latest/server_admin/#proc-creating-oidc-client_server_administration_guide).

Additionally, refer to [**`Grafana Documentation: Configure Keycloak OAuth2 Authentication`**](https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/keycloak/) to ensure proper configuration alignment.

**Steps**:

1. **Create Realm Roles Corresponding to Grafana's Roles**:

   - Define roles such as `admin`, `editor`, and `viewer` within the picluster realm to match Grafana's role structure. Refer to [**`Grafana's Roles`**](https://grafana.com/docs/grafana/latest/administration/roles-and-permissions/).

2. **Create a New OIDC Client in Keycloak**:

   - **Navigate to**: `Clients` ➜ `Create Client`

   - **Basic Configuration**:
     - **Client Type**: `OpenID Connect`
     - **Client ID**: `grafana`
     - **Name**: `Grafana`
     - **Description**: `Grafana`
     - **Always display in UI**: `On`
     - **Click**: `Next`

<p align="center">
    <img alt="grafana-keycloak-general-settings"
    src="../resources/monitoring/grafana-keycloak-general-settings.jpg"
    width="90%"
    height="%">
</p>

   - **Capability Configuration**:
     - **Client Authentication**: `On`
     - **Authentication Flow**:
       - **Standard Flow**: `Selected`
       - **Direct Access Grants**: `Selected`
   - **Click**: `Next`

<p align="center">
    <img alt="grafana-keycloak-capability-config"
    src="../resources/monitoring/grafana-keycloak-capability-config.jpg"
    width="90%"
    height="%">
</p>

   - **Logging Settings**:
     - **Root URL**: `https://monitoring.picluster.quantfinancehub.com/grafana`
     - **Home URL**: `https://monitoring.picluster.quantfinancehub.com/grafana`
     - **Valid Redirect URIs**: `https://monitoring.picluster.quantfinancehub.com/grafana/login/generic_oauth`
     - **Web Origins**: `https://monitoring.picluster.quantfinancehub.com/grafana`
     - **Save Configuration**

<p align="center">
    <img alt="grafana-keycloak-login-settings"
    src="../resources/monitoring/grafana-keycloak-login-settings.jpg"
    width="90%"
    height="%">
</p>

3. **Retrieve Grafana Client Credentials**:

   - **Navigate to**: `Credentials` tab within the Grafana client.
   - **Note**: Locate the `Client Secret` for the Grafana client.

<p align="center">
    <img alt="grafana-keycloak-client-credentials"
    src="../resources/monitoring/grafana-keycloak-client-credentials.jpg"
    width="90%"
    height="%">
</p>

4. **Configure a Dedicated Role Mapper for the Client**:

   - **Navigate to**: `Clients` ➜ `grafana client` ➜ `Client Scopes`

<p align="center">
    <img alt="grafana-keycloak-client-scopes"
    src="../resources/monitoring/grafana-keycloak-client-scopes.jpg"
    width="90%"
    height="%">
</p>

   - **Access Dedicated Mappers Pane**:
     - Click on `grafana-dedicated` under `Assigned Client Scope`.
   - **Add a New Mapper**:
     - **Type**: `User Realm Role`
     - **Configure as follows**:
       - **Name**: `roles`
       - **Multivalued**: `On`
       - **Token Claim Name**: `roles`
       - **Claim JSON Type**: `String`
       - **Add to ID token**: `On`
       - **Add to access token**: `On`
       - **Add to userinfo**: `On`
       - **Add to token introspection**: `On`
     - **Save Configuration**

<p align="center">
    <img alt="grafana-keycloak-configure-a-new-mapper"
    src="../resources/monitoring/grafana-keycloak-configure-a-new-mapper.jpg"
    width="90%"
    height="%">
</p>

<p align="center">
    <img alt="grafana-keycloak-configure-a-new-mapper-list"
    src="../resources/monitoring/grafana-keycloak-configure-a-new-mapper-list.jpg"
    width="90%"
    height="%">
</p>

<p align="center">
    <img alt="grafana-keycloak-add-mapper"
    src="../resources/monitoring/grafana-keycloak-add-mapper.jpg"
    width="90%"
    height="%">
</p>

5. **Create Users and Assign Roles**:
   - **Navigate to**: `Users` ➜ `Add User`
   - **Assign Roles**: Assign any of the roles (`admin`, `editor`, `viewer`) created in Step 1 to the respective users.

### Configuring SSO for Grafana SSO

- Ensure that the `client_secret` is stored in Vault at a known path. Based on your policies, you can store it at `secret/data/grafana/client_secret`.

```bash
vault kv put secret/grafana client_secret=<supersecret>
# Where <supersecret> in step 3 above
```

- Create an `ExternalSecret` that instructs the External Secrets Operator to fetch the `client_secret` from Vault and create a Kubernetes Secret. This `ExternalSecret` will create a Kubernetes Secret named `grafana-client-secret` with the key `GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET`.

```yaml
# grafana-client-secret-externalsecret.yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: grafana-client-secret
  namespace: monitoring
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: ClusterSecretStore
  target:
    name: grafana-client-secret
    creationPolicy: Owner
  data:
    - secretKey: GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET
      remoteRef:
        key: secret/grafana
        property: client_secret
```

- Apply the ExternalSecret

```bash
kubectl apply -f grafana-client-secret-externalsecret.yaml
```

- Perform somesanity checks

```bash
kubectl get ExternalSecret -n monitoring
kubectl describe ExternalSecret grafana-client-secret -n monitoring
kubectl get secret grafana-client-secret -n monitoring
kubectl describe secret grafana-client-secret -n monitoring
```

- Update`Grafana's` helm chart, `prometheus-values.yaml`, to reference the secret and authenticate leveraging `Keycloak` with `OAuth2-Proxy`

```yaml
# Grafana configuration
grafana:
  grafana.ini:
    server:
      domain: monitoring.picluster.quantfinancehub.com
      root_url: "https://%(domain)s/grafana/"
      serve_from_sub_path: false
    # SSO configuration
    auth.generic_oauth:
      enabled: true
      name: Keycloak-OAuth
      allow_sign_up: true
      client_id: grafana
      # client_secret is provided via environment variable; do not include it here
      scopes: openid email profile offline_access roles
      email_attribute_path: email
      login_attribute_path: username
      name_attribute_path: full_name
      auth_url: https://sso.picluster.quantfinancehub.com/realms/picluster/protocol/openid-connect/auth
      token_url: https://sso.picluster.quantfinancehub.com/realms/picluster/protocol/openid-connect/token
      api_url: https://sso.picluster.quantfinancehub.com/realms/picluster/protocol/openid-connect/userinfo
      role_attribute_path: contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'
      signout_redirect_url: https://sso.picluster.quantfinancehub.com/realms/picluster/protocol/openid-connect/logout?client_id=grafana&post_logout_redirect_uri=https%3A%2F%2Fmonitoring.picluster.quantfinancehub.com%2Fgrafana%2Flogin%2Fgeneric_oauth
  # Admin user password (change this to a secure password)
  adminPassword: "admin_password"
  # List of Grafana plugins to be installed
  plugins:
    - grafana-piechart-panel
  # ServiceMonitor configuration
  serviceMonitor:
    labels:
      release: kube-prometheus-stack
    relabelings:
      - sourceLabels: [__address__]
        action: replace
        targetLabel: job
        replacement: grafana
  # Additional data sources
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki-gateway.logging.svc.cluster.local
  # Sidecar configuration for dashboards
  sidecar:
    dashboards:
      searchNamespace: ALL
  # Reference the secret containing the client_secret
  extraEnv:
    - name: GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET
      valueFrom:
        secretKeyRef:
          name: grafana-client-secret
          key: GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET
```

- Apply the upgraded helm chart

```bash
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  -f prometheus-values.yaml \
  -n monitoring
```

### Provisioning Grafana Dashboards Automatically

[**`Grafana dashboards`**](https://grafana.com/docs/grafana/latest/dashboards/) can be set up through provider definitions in YAML files located in the provisioning directory (**`/etc/grafana/provisioning/dashboards`**). These files specify the directory from which to load JSON-formatted dashboards. For more details, refer to the Grafana Tutorial on [**`Provisioning Dashboards and Data Sources`**](https://grafana.com/tutorials/provision-dashboards-and-data-sources/).

When deploying Grafana in Kubernetes via Helm, dashboards can be provisioned automatically by enabling a sidecar container provisioner. The Grafana Helm chart generates a **`/etc/grafana/provisioning/dashboard/provider.yml`** file, instructing Grafana to load all JSON dashboards from **`/tmp/dashboards`**.

The configuration in **`provider.yml`** is:

```yaml
apiVersion: 1
providers:
  - name: 'sidecarProvider'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    allowUiUpdates: false
    updateIntervalSeconds: 30
    options:
      foldersFromFilesStructure: false
      path: /tmp/dashboards
```

With the sidecar provider activated, Grafana dashboards are automatically provisioned by creating ConfigMap resources that contain the dashboard's JSON definition. The provisioning sidecar container must be enabled to monitor these ConfigMaps in real time and copy them to the provisioning directory (**`/tmp/dashboards`**).

The **`kube-prometheus-stack`**, by default, configures the Grafana provisioning sidecar to look for new ConfigMaps labeled with **`grafana_dashboard`**. The default Helm chart values for the kube-prometheus-stack are:

```yaml
grafana:
  sidecar:
    dashboards:
      SCProvider: true
      enabled: true
      folder: /tmp/dashboards
      label: grafana_dashboard
```

To automatically provision a new dashboard, create a **`ConfigMap`** resource labeled with **`grafana_dashboard: "1"`** and include the JSON file content as **`data`**:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sample-grafana-dashboard
  labels:
     grafana_dashboard: "1"
data:
  dashboard.json: |-
    [json_file_content]
```

To enable the search for ConfigMaps across all namespaces (the default is limited to the Grafana namespace), additional Helm chart configuration is required:

```yaml
grafana:
  sidecar:
    dashboards:
      searchNamespace: ALL
```

Following this setup, the kube-prometheus-stack Helm chart automatically deploys a set of dashboards for monitoring metrics from Kubernetes processes and Node Exporter.

However, for K8s components disabled in the chart configuration, the corresponding dashboards are not deployed and need to be added manually. Refer to the lis of [**`kube-prometheus-stack grafana dashboards`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14)

To manually add these dashboards or to retrieve all deployed dashboards, use the following command:

```bash
kubectl get cm -l "grafana_dashboard=1" -n monitoring
```

> 🚨 **Important Note**
>
> Many [**`Grafana community dashboards`**](https://grafana.com/grafana/dashboards/) are exported from a running instance and include an input variable (**`DS_PROMETHEUS`**) referenced in all dashboard panels. When provisioning these dashboards automatically, it possible to encounter an error stating **`"Datasource named ${DS_PROMETHEUS} was not found."`** This is due to an open Grafana issue requesting support for dashboard variables in provisioning. 
>
> A workaround is to modify the JSON files before inserting them into the ConfigMap YAML file, detecting the **`DS_PROMETHEUS`** datasource and adjusting the **`templating.list`** key accordingly.
>
> ```json
> "templating": {
>     "list": [
>       {
>         "hide": 0,
>         "label": "datasource",
>         "name": "DS_PROMETHEUS",
>         "options": [],
>         "query": "prometheus",
>         "refresh": 1,
>         "regex": "",
>         "type": "datasource"
>       },
>     ...
> ```

### Automatic Provisioning of Grafana DataSources

Grafana allows for the configuration of [**`data sources`**](https://grafana.com/docs/grafana/latest/datasources/) through YAML files situated in the provisioning directory (**`/etc/grafana/provisioning/datasources`**). For a comprehensive guide, refer to the Grafana Tutorial on [**`Provisioning Dashboards and Data Sources`**](https://grafana.com/tutorials/provision-dashboards-and-data-sources/).

In a Kubernetes deployment of Grafana, data source configuration files can be introduced from ConfigMaps. This process is facilitated by a sidecar container that translocates these ConfigMaps into its provisioning directory.

For detailed instructions on enabling and utilizing this sidecar container, see the [**`“Grafana chart documentation: Sidecar for Datasources”`**](https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-datasources).

By default, the **`kube-prometheus-stack`** activates the Grafana data source sidecar to monitor for new ConfigMaps tagged with the label grafana_datasource. The configuration for this is as follows:

```yaml
sidecar:
  datasources:
    enabled: true
    defaultDatasourceEnabled: true
    uid: prometheus
    annotations: {}
    createPrometheusReplicasDatasources: false
    label: grafana_datasource
    labelValue: "1"
    exemplarTraceIdDestinations: {}
```

Below is an example of a ConfigMap automatically generated by the **`kube-prometheus-stack`**, defining the data source that connects Grafana to the Prometheus server (named "**`Prometheus`**"):

It can be retreived from PiKube Kunbernetes cluster

```bash
kubectl --kubeconfig=/home/pi/.kube/config.yaml get configmaps -n monitoring
kubectl --kubeconfig=/home/pi/.kube/config.yaml get configmap <ConfigMap-Name> -n monitoring -o yaml # similar to kube-prometheus-stack-grafana-datasource 
```

```yaml
apiVersion: v1
data:
  datasource.yaml: |-
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      uid: prometheus
      url: http://kube-prometheus-stack-prometheus.monitoring:9090/
      access: proxy
      isDefault: true
      jsonData:
        httpMethod: POST
        timeInterval: 30s
    - name: Alertmanager
      type: alertmanager
      uid: alertmanager
      url: http://kube-prometheus-stack-alertmanager.monitoring:9093/
      access: proxy
      jsonData:
        handleGrafanaManagedAlerts: false
        implementation: prometheus
    - name: Loki
      type: loki
      url: http://loki-gateway.logging.svc.cluster.local
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:47Z"
  labels:
    app: kube-prometheus-stack-grafana
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    grafana_datasource: "1"
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-grafana-datasource
  namespace: monitoring
  resourceVersion: "1344466"
  uid: 78b4f328-50dd-40d8-b03a-27b3a8da344b
```

This ConfigMap is distinguished by the **`grafana_datasource`** label, which ensures it's recognized and imported by the sidecar container into Grafana’s provisioning directory, thereby streamlining the setup process.

### Prometheus Node Exporter Deployment

The Prometheus Node Exporter, deployed via its [**`Helm chart`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter), is included as a subchart within the `kube-prometheus-stack` Helm chart. This deployment facilitates the Node Exporter's installation across all cluster nodes as a DaemonSet.

The `kube-prometheus-stack` Helm chart utilizes the **`prometheus-node-exporter`** values to configure the Node Exporter chart. By default, the `prometheus-values.yaml` file in the kube-prometheus-stack contains specific configurations that remain unchanged in the [**`standard installation procedure`**](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml).

Here's an outline of the default settings for the Prometheus Node Exporter within the kube-prometheus-stack:

```yaml
prometheus-node-exporter:
  # Namespace override
  namespaceOverride: ""
  
  # Pod labels
  podLabels:
    jobLabel: node-exporter  # Label for serviceMonitor to match standard usage in rules and Grafana dashboards
  
  # Additional arguments to exclude certain mount points and file types
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  
  # Service and monitoring configurations
  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true
      jobLabel: jobLabel
      interval: ""  # Scrape interval
      scrapeTimeout: ""  # Scrape request timeout
      proxyUrl: ""  # Proxy URL for scraping
      metricRelabelings: []  # Metric relabeling configurations
      relabelings: []  # Relabeling configurations before scraping
  
  # RBAC configuration
  rbac:
    pspEnabled: false  # Create PodSecurityPolicies for node-exporte
```

The default configuration primarily focuses on excluding specific mount points and file types from monitoring (**`extraArgs`**) and establishes a ServiceMonitor object to initiate metric scraping from the exporter.

Metrics from the Prometheus Node Exporter are available at the **`/metrics`** endpoint on TCP port 9100 of each DaemonSet Pod. This setup ensures comprehensive node-level metrics collection across the Kubernetes cluster, aiding in detailed monitoring and analysis.

### Deployment of Kube State Metrics

The [**`Prometheus Kube State Metrics`**](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics), integrated via its dedicated Helm chart, is included as a subchart in the `kube-prometheus-stack` Helm chart deployment.

This chart facilitates the deployment of the [**`kube-state-metrics agent`**](https://github.com/kubernetes/kube-state-metrics). Kube-state-metrics (KSM) is a straightforward service that monitors the Kubernetes API server and generates metrics reflecting the status of various Kubernetes objects.

The configuration for **`kube-state-metrics`** within the kube-prometheus-stack is provided via the **`kube-state-metrics value`**, which is passed to the kube-state-metrics chart to dictate its setup and behavior.

The metrics from `kube-state-metrics` are made available on TCP port 8080 at the **`/metrics`** endpoint, providing a comprehensive set of data about the state and performance of different Kubernetes resources and objects.

## Monitoring K3S and Cluster Services

This section outlines the steps required to enable Prometheus monitoring for K3S components and various deployed cluster services.

The process involves setting up Kubernetes resources such as **`Services`**, **`Endpoints`**, **`ServiceMonitors`**, **`PodMonitors`**, and **`Probes`**. These are necessary to configure Prometheus' service discovery and monitoring capabilities. Additionally, it details how to import dashboards in JSON format into Grafana for a visual representation of the metrics from each specific service.

> 📌 **Note**
>
> *To provision the dashboards mentioned in the subsequent sections, a ConfigMap should be created for each dashboard (JSON file). This can be done following the process previously described:*
>
> ```yaml
> apiVersion: v1
> kind: ConfigMap
> metadata:
>   name: sample-grafana-dashboard
>   labels:
>      grafana_dashboard: "1"
> data:
>   dashboard.json: |-
>     [json_file_content]
> ```

### K3S Components Monitoring

The [**`Kubernetes Documentation on System Metrics`**](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/) outlines various components that expose metrics in a Prometheus-friendly format:

- **`kube-controller-manager`**: Metrics endpoint at TCP 10257
- **`kube-proxy`**: Metrics endpoint at TCP 10249
- **`kube-apiserver`**: Metrics at Kubernetes API port TCP 6443
- **`kube-scheduler`**: Metrics endpoint at TCP 10259
- **`kubelet`**: Metrics at **`/metrics`**, **`/metrics/cadvisor`**, **`/metrics/resource`**, and **`/metrics/probes`** endpoints at TCP 10250

> 📌 **Note**
>
> *From Kubernetes release 1.22, TCP ports exposed by kube-scheduler and kube-controller-manager have changed. Additionally, HTTPS authenticated connections are now required, necessitating authorized service accounts for metrics access.*
>
>🚨 **Important Note**
>
> By default, K3S components do not expose their metrics endpoints for remote querying. To alter this and allow for remote metrics collection, include the following arguments in the K3S installation:
>
> ```bash
> --kube-controller-manager-arg 'bind-address=0.0.0.0' 
> --kube-proxy-arg 'metrics-bind-address=0.0.0.0'
> --kube-scheduler-arg 'bind-address=0.0.0.0'
> ```

However, the default resources created by `kube-prometheus-stack` to scrape metrics from all Kubernetes components are not suitable for a K3S cluster due to its unique behavior concerning metrics exposure. K3S emits the same metrics across all ports for each node, leading to potential duplicates in Prometheus.

To avoid duplicate metrics and unnecessary resource consumption, it's recommended to scrape only the kubelet metrics endpoints (TCP 10250). This includes **`/metrics`**, **`/metrics/cadvisor`**, **`/metrics/resource`**, and **`/metrics/probes`**.

As a result, the monitoring of Kubernetes components has been disabled in the `kube-prometheus-stack` chart configuration, and a manual resource configuraiton is needed to scrape metrics from the kubelet endpoints.

```yaml
# Disabling default Kubernetes service monitoring
kubelet: { enabled: false }
kubeApiServer: { enabled: false }
kubeControllerManager: { enabled: false }
kubeScheduler: { enabled: false }
kubeProxy: { enabled: false }
kubeEtcd: { enabled: false }

# Disabling default K8S Prometheus Rules
defaultRules:
  create: true
  rules:
    etcd: false
    k8s: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeScheduler: false
```

Create a **`prometheus-k3s-metrics-service.yaml`** manifest file to set up a headless service for K3S metrics. This service should be a **`ClusterIP`** with **`clusterIP: None`**, allowing Prometheus to discover individual pods behind the service. Since metrics are emitted by the k3s process rather than pods, the service must be defined without a `selector`, and `endpoints` must be explicitly defined.

```yaml
---
# Headless service for K3S metrics. No selector.
apiVersion: v1
kind: Service
metadata:
  name: k3s-metrics-service
  labels:
    app.kubernetes.io/name: kubelet
  namespace: kube-system
spec:
  clusterIP: None
  ports:
  - name: https-metrics
    port: 10250
    protocol: TCP
    targetPort: 10250
  type: ClusterIP
---
# Endpoints for the headless service without a selector.
apiVersion: v1
kind: Endpoints
metadata:
  name: k3s-metrics-service
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.0.0.10
  - ip: 10.0.0.11
  - ip: 10.0.0.12
  - ip: 10.0.0.13
  - ip: 10.0.0.14
  - ip: 10.0.0.15
  - ip: 10.0.0.16
  ports:
  - name: https-metrics
    port: 10250
    protocol: TCP
```

Next, create a **`ServiceMonitor`** resource, named **`prometheus-k3s-servicemonitor.yaml`** to let Prometheus discover these targets. This single ServiceMonitor should enable the collection of all Kubernetes component metrics from the unique port TCP 10250, including necessary relabeling and dropping rules.

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    release: kube-prometheus-stack
  name: k3s-monitoring
  namespace: monitoring
spec:
  endpoints:
  # /metrics endpoint
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    metricRelabelings:
    # apiserver
    - action: drop
      regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
      sourceLabels:
      - __name__
      - le
    port: https-metrics
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
  # /metrics/cadvisor
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    metricRelabelings:
    - action: drop
      regex: container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_memory_(mapped_file|swap)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_(file_descriptors|tasks_state|threads_max)
      sourceLabels:
      - __name__
    - action: drop
      regex: container_spec.*
      sourceLabels:
      - __name__
    path: /metrics/cadvisor
    port: https-metrics
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    # /metrics/probes
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    honorLabels: true
    path: /metrics/probes
    port: https-metrics
    relabelings:
    - action: replace
      sourceLabels:
      - __metrics_path__
      targetLabel: metrics_path
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app.kubernetes.io/name: kubelet
```

> 📌 **Note**
>
> This ServiceMonitor configures a single Prometheus scraping job with the label **`job="kubelet"`**. Most Grafana dashboards use kubelet metrics, and their PromQL queries filter metrics by the label **`job="kubelet"`**.
> When the monitoring of Kubernetes components is deactivated within the kube-prometheus-stack, the associated Prometheus rules for Kubernetes components are not installed. Moreover, these rules are not applicable for K3S as they include PromQL queries that filter metrics based on job labels like “apiserver,” “kubelet,” etc.

By default, the kube-prometheus-stack generates various PrometheusRules resources. These are collectively contained within a single manifest file named [**`kubernetesControlPlane-prometheusRule.yaml`**](https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-prometheusRule.yaml) in the prometheus-operator's source repository.

To adapt these rules for K3S, modifications are required in the yaml file to change the job labels:

- Download and update the job labels from:

  - **`job="apiserver"`**
  - **`job="kube-proxy"`**
  - **`job="kube-scheduler"`**
  - **`job="kube-controller-manager"`**

To simply:

- **`job="kubelet"`**

This aligns the rules with the K3S metrics that are all emitted under the **`kubelet`** job label.

To ensure these modified rules are recognized and used by the Prometheus Operator, replace specific label to the metadata section of the PrometheusRule resource:

From

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/name: kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus
    prometheus: k8s
    role: alert-rules
  name: kubernetes-monitoring-rules
```

To

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    release: kube-prometheus-stack
  name: kubernetes-monitoring-rules
```

This label assists the Prometheus Operator in identifying and applying the rules appropriately.

- Apply manifest files

```bash
kubectl apply -f prometheus-k3s-metrics-service.yaml prometheus-k3s-servicemonitor.yaml prometheusrule.yaml
```

Check targets are discoverable in Prometheu UI under `Status` ➜ `Targets`

- `https://monitoring.picluster.quantfinancehub.com/prometheus/targets`

#### coreDNS Monitoring

By default, the kube-prometheus-stack Helm chart enables CoreDNS monitoring.

```yaml
coreDns:
  enabled: true
  service:
    port: 9153
    targetPort: 9153
    ...
```

It creates a `Service` named `kube-prometheus-stack-coredns` in the `kube-system` namespace that points to the CoreDNS Pod. Here's how to retreive it and an overview of the configuration:

```bash
kubectl get svc kube-prometheus-stack-coredns -n kube-system -o yaml
```

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:47Z"
  labels:
    app: kube-prometheus-stack-coredns
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    jobLabel: coredns
    release: kube-prometheus-stack
  name: kube-prometheus-stack-coredns
  namespace: kube-system
  resourceVersion: "1344504"
  uid: b4453d96-1abe-4b0b-baea-36d5323540e9
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http-metrics
    port: 9153
    protocol: TCP
    targetPort: 9153
  selector:
    k8s-app: kube-dns
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

It also creates a `ServiceMonitor` named `kube-prometheus-stack-coredns` in the `kube-system` namespace. Here's how to retrieve it and an overview of the configuration:

```bash
kubectl get servicemonitor kube-prometheus-stack-coredns -n monitoring -o yaml
```

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  annotations:
    meta.helm.sh/release-name: kube-prometheus-stack
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2023-12-22T19:54:48Z"
  generation: 1
  labels:
    app: kube-prometheus-stack-coredns
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 55.5.0
    chart: kube-prometheus-stack-55.5.0
    heritage: Helm
    release: kube-prometheus-stack
  name: kube-prometheus-stack-coredns
  namespace: monitoring
  resourceVersion: "1344626"
  uid: b8db049f-591b-4cf1-9ee0-45be43b72549
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    port: http-metrics
  jobLabel: jobLabel
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      app: kube-prometheus-stack-coredns
      release: kube-prometheus-stack
```

#### Integrate and Modify Grafana Dashboards with K3S

While `kube-prometheus-stack` installs Grafana dashboards for various Kubernetes components, a manual installation might be needed if their monitoring is disabled in the Helm chart configuration.

Dashboards to Download from [**`Grafana.com`**](https://grafana.com/):

- **`kubelet`**: ID [**`16361`**](https://grafana.com/grafana/dashboards/16361-kubernetes-kubelet/)
- **`apiserver`**: ID [**`12654`**](https://grafana.com/grafana/dashboards/12654-kubernetes-api-server/)
- **`etcd`**: ID [**`16359`**](https://grafana.com/grafana/dashboards/16359-etcd/)
- **`kube-scheduler`**: ID [**`12130`**](https://grafana.com/grafana/dashboards/12130-kubernetes-scheduler/)
- **`kube-controller-manager`**: ID [**`12122`**](https://grafana.com/grafana/dashboards/12122-kubernetes-controller-manager/)
- **`kube-proxy`**: ID [**`12129`**](https://grafana.com/grafana/dashboards/12129-kubernetes-proxy/)

**Modifications for K3S**:

The downloaded Grafana dashboards might need modifications as the default promQL queries use job name labels (like `kube-scheduler`, `kube-proxy`, `apiserver`, etc.) which are not used in K3S configuration. Instead, K3S typically has a single scraping job (`kubelet`) configured to scrape metrics from all components.

- Download and update the JSON job labels from:

- **`job="apiserver"`**
- **`job="kube-proxy"`**
- **`job="kube-scheduler"`**
- **`job="kube-controller-manager"`**

To simply:

- **`job="kubelet"`**

- Create a `ConfigMap` for the modified dashboards

```yaml
# grafana-apiserver-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apiserver-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "K3S Components"
data:
  apiserver-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

```yaml
# grafana-kube-proxy-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "K3S Components"
data:
  kube-proxy-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

```yaml
# grafana-kube-scheduler-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-scheduler-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "K3S Components"
data:
  kube-scheduler-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

```yaml
# grafana-kube-controller-manager-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-controller-manager-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "K3S Components"
data:
  kube-controller-manager-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

- Deploy the manifests

```bash
kubectl apply -f grafana-apiserver-dashboard-configmap.yaml grafana-kube-proxy-dashboard-configmap.yaml grafana-kube-scheduler-dashboard-configmap.yaml grafana-kube-controller-manager-dashboard-configmap.yaml
```

When incorporating the JSON templates the yaml files make sure to open the uam files in a text editor:

- Press `Ctrl + H` to open the Replace dialog.
- Enter `\t` in the `"Find"` field.
- Enter the appropriate number of spaces (here `4`) in the `"Replace"` field.
- Enable `"Use Regular Expression"` (the .* icon).
- Click `"Replace All"`.

Ensure to review the modified dashboards in Grafana to confirm they are reflecting the correct metrics after the changes.

### Monitoring Ingress NGINX

To automatically discover the Ingress NGINX metrics endpoint as a Prometheus target, you'll utilize the Prometheus Custom Resource Definition (CRD) `ServiceMonitor`.

Before creating the `ServiceMonitor`, it's essential to identify the exact labels assigned to your NGINX Ingress controller service. These labels ensure that Prometheus correctly targets the service for scraping metrics.

- List `Services` in the NGINX Namespace

```bash
kubectl get svc -n nginx
```

- Retrieve the labels associated with the NGINX Ingress controller service to ensure accurate configuration

```bash
kubectl get svc ingress-nginx-controller -n nginx --show-labels
```

- Note the key `Labels` and create the `ServiceMonitor` manifest `prometheus-nginx-servicemonitor.yaml`

```yaml
# prometheus-nginx-servicemonitor.yaml
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-ingress-monitoring
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  endpoints:
  - port: metrics
    path: /metrics
    scheme: http
    interval: 30s
    honorLabels: true
  selector:
    matchLabels:
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/component: controller
  namespaceSelector:
    matchNames:
      - nginx
```

::: warning

The `app.kubernetes.io/name` label on the service will be utilized as the Prometheus `jobLabel`. This ensures that Prometheus correctly associates metrics with the appropriate job.

:::

- Apply the ServiceMonitor manifest

```bash
kubectl apply -f prometheus-nginx-servicemonitor.yaml
```

#### Ingress NGINX Grafana Dashboard

- Download the Ingress NGINX Grafana dashboard in JSON format from the [**`Kubernetes Ingress-NGINX GitHub repository`**](https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/nginx.json).

- Build `grafana-nginx-dashboard-configmap.yaml`

```yaml
# grafana-nginx-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "NGINX Ingress"
data:
  nginx-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

- Apply the ConfigMap manifest

```bash
kubectl apply -f grafana-nginx-dashboard-configmap.yaml
```

### Monitoring Longhorn

According to the official [**`Longhorn documentation`**](https://chatgpt.com/c/674f03f0-4fb8-8004-ba32-a6690f1f9f7d), the **Longhorn Backend service** directs traffic to the set of **Longhorn manager** pods. These manager pods expose Prometheus metrics at the endpoint:

```css
http://LONGHORN_MANAGER_IP:PORT/metrics
```

The backend endpoint is **already configured** to expose Prometheus metrics.

To automatically discover the Longhorn metrics endpoint as a Prometheus target, you'll use the Prometheus Custom Resource Definition (CRD) `ServiceMonitor`.

- List `Services` in Longhorn Namespace

```bash
kubectl get svc -n longhorn-system
```

- Retrieve the labels associated with the NGINX Ingress controller service to ensure accurate configuration

```bash
kubectl get svc longhorn-backend -n nginx --show-labels
```

- Note the key `Labels` and create the `ServiceMonitor` manifest `prometheus-nginx-servicemonitor.yaml`

```yaml
# prometheus-longhorn-servicemonitor.yaml
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: longhorn-monitoring
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  selector:
    matchLabels:
      app.kubernetes.io/name: longhorn-manager
  namespaceSelector:
    matchNames:
      - longhorn-system
  endpoints:
    - port: metrics
      path: /metrics
      scheme: http
      interval: 30s
      honorLabels: true
```

::: warning

The `app.kubernetes.io/name` label on the service is utilized as Prometheus's jobLabel. This ensures that Prometheus correctly associates metrics with the appropriate job.

:::

- Apply the ServiceMonitor manifest

```bash
kubectl apply -f prometheus-longhorn-servicemonitor.yaml
```

#### Longhorn Grafana Dashboard

- Download the Longhorn Grafana dashboard in JSON format from Grafana website: [**`Dashboard id: 13032`**](https://grafana.com/grafana/dashboards/13032).

- Build `grafana-longhorn-dashboard-configmap.yaml` using the methodology described in [**`Integrate and Modify Grafana Dashboards with K3S`**](#integrate-and-modify-grafana-dashboards-with-k3s)

```yaml
# grafana-longhorn-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: longhorn-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "Longhorn"
data:
  longhorn-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

- Apply the ConfigMap manifest

```bash
kubectl apply -f grafana-longhorn-dashboard-configmap.yaml
```

### Monitoring Minio

For comprehensive details, refer to [**`MinIO's official guide on "Collect MinIO Metrics Using Prometheus"`**](https://docs.min.io/minio/baremetal/monitoring/metrics-alerts/collect-minio-metrics-using-prometheus.html).

For detailed instructions, refer to [**`MinIO's documentation on collecting metrics using Prometheus`**](https://docs.min.io/minio/baremetal/monitoring/metrics-alerts/collect-minio-metrics-using-prometheus.html).

- From `blueberry-master` nodes, generate a bearer token to enable Prometheus to access MinIO metrics

```bash
mc admin prometheus generate <alias>
```

Where `<alias>` is `PiKubeS3Cluster`. The command outputs a configuration like the following

```yaml
scrape_configs:
- job_name: minio-job
  bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoicGljbHVzdGVyIiwiZXhwIjo0ODg3MTI4MTgyfQ.ToRsc6QMm4_HAvr6tNnkOe4_MjwuZZ4cfkpRkol2y7SEV8FK3v7akxTXoIBRALvITY7l56WeLPCcGMfE43233w
  metrics_path: /minio/v2/metrics/cluster
  scheme: https
  static_configs:
  - targets: [s3.picluster.quantfinancehub.com]
```

**Details**:

`bearer_toke`n: Authentication token for Prometheus.
`metrics_path`: Path where metrics are exposed on the MinIO server (port 9091).

- Create a Kubernetes service for Prometheus to scrape MinIO metrics. This will be a headless service without a selector. Define endpoints explicitly in `prometheus-minio-metrics-service.yaml`

```yaml
---
# Headless service for MinIO metrics
apiVersion: v1
kind: Service
metadata:
  name: minio-metrics-service
  labels:
    app.kubernetes.io/name: minio
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 9091
      protocol: TCP
      targetPort: 9091
  type: ClusterIP
---
# Explicit endpoint for the headless service
apiVersion: v1
kind: Endpoints
metadata:
  name: minio-metrics-service
  namespace: kube-system
subsets:
  - addresses:
      - ip: 10.0.0.10  # blueberry-master IP
    ports:
      - name: http-metrics
        port: 9091
        protocol: TCP
```

- To enable Prometheus to discover MinIO metrics, create, `prometheus-minio-servicemonitor.yaml`, a Secret resource containing the bearer token and a ServiceMonitor for Prometheus

```yaml
---
# Bearer token for MinIO metrics
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: minio-monitor-token
  namespace: monitoring
data:
  token: <minio_bearer_token | b64encode>
---
# Prometheus ServiceMonitor for MinIO metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: minio
    release: kube-prometheus-stack
  name: minio-servicemonitor
  namespace: monitoring
spec:
  jobLabel: app.kubernetes.io/name
  endpoints:
    - port: http-metrics
      path: /minio/v2/metrics/cluster
      scheme: https
      tlsConfig:
        insecureSkipVerify: true
      bearerTokenSecret:
        name: minio-monitor-token
        key: token
  namespaceSelector:
    matchNames:
      - kube-system
  selector:
    matchLabels:
      app.kubernetes.io/name: minio
```

- Apply the created manifests

```bash
kubectl apply -f prometheus-minio-metrics-service.yaml
kubectl apply -f prometheus-minio-servicemonitor.yaml
```

#### Minio Grafana Dashboard

To visualize MinIO metrics, use a Grafana dashboard. Download a sample dashboard from [**`Grafana's dashboard repository: Dashboard ID: 13502`**](https://grafana.com/grafana/dashboards/13502).

```yaml
# grafana-minio-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: minio-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "Minio"
data:
  minio-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

- Apply the ConfigMap manifest

```bash
kubectl apply -f grafana-longhorn-dashboard-configmap.yaml
```

### Elasticsearch Monitoring

To collect Elasticsearch metrics in Prometheus format, you need to install the [**`prometheus-elasticsearch-exporter`**](https://github.com/prometheus-community/elasticsearch_exporter). Refer to the [**`"Installing Prometheus Elasticsearch Exporter"`**](../9-monitoring/5-log-analytics-elasticsearch-kibana.md) documentation for detailed instructions.

This exporter exposes a `/metrics` endpoint on port `9108`.

To automatically discover the Elasticsearch exporter’s metrics endpoint as a Prometheus target, use the `ServiceMonitor` Custom Resource Definition (CRD).

- Create a manifest file named `prometheus-elasticsearch-servicemonitor.yaml`

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: elasticsearch-servicemonitor
  namespace: monitoring
  labels:
    app: prometheus-elasticsearch-exporter
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: prometheus-elasticsearch-exporter
  namespaceSelector:
    matchNames:
      - logging
  endpoints:
    - port: http
      path: /metrics
```

- Apply the manifest

```bash
kubectl apply -f prometheus-elasticsearch-servicemonitor.yaml
```

#### Elasticsearch Grafana Dashboard

- Download the Elasticsearch Grafana dashboard in JSON format from the [**`prometheus-elasticsearch-exporter repository`**](https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/nginx.json).

- Build `grafana-elasticsearch-dashboard-configmap.yaml`

```yaml
# grafana-elasticsearch-dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
  annotations:
    grafana_folder: "Elasticsearch"
data:
  elasticsearch-dashboard.json: |-
    {
      // Paste the entire modified JSON content here
    }
```

- Apply the ConfigMap manifest

```bash
kubectl apply -f grafana-elasticsearch-dashboard-configmap.yaml
```